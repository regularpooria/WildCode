{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import math\n",
    "from utils.extractor import walk_and_extract_cwe\n",
    "\n",
    "lang_to_ext = {\n",
    "    'c': 'c',\n",
    "    'cpp': 'cpp',\n",
    "    'python': 'py',\n",
    "    'java': 'java',\n",
    "    'javascript': 'js',\n",
    "    'php': 'php',\n",
    "    \"csharp\": \"cs\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_conversations(file_path):\n",
    "    # Structure: {language: {conversation_hash: count, 'total_unique': count, 'total_occurrences': count}}\n",
    "    language_data = defaultdict(lambda: {'unique_conversations': set(), 'total_occurrences': 0})\n",
    "\n",
    "    with open(file_path, mode='r') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "\n",
    "        for row in csv_reader:\n",
    "            conversation_hash = row['conversation_hash']\n",
    "            language = row['language']\n",
    "\n",
    "            # Track data per language\n",
    "            lang_data = language_data[language]\n",
    "\n",
    "            # Add to unique conversations if not already present\n",
    "            if conversation_hash not in lang_data['unique_conversations']:\n",
    "                lang_data['unique_conversations'].add(conversation_hash)\n",
    "\n",
    "            # Increment total occurrences\n",
    "            lang_data['total_occurrences'] += 1\n",
    "\n",
    "    # Convert sets to counts and prepare final output\n",
    "    result = {}\n",
    "    for language, data in language_data.items():\n",
    "        result[language] = {\n",
    "            'unique_conversations': len(data['unique_conversations']),\n",
    "            'total_occurrences': data['total_occurrences']\n",
    "        }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_files = os.listdir(\"files/c/codes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20.574345742720237, 28.810912040148537)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_code_lines_stats(c_files):\n",
    "    line_counts = []\n",
    "\n",
    "    for c_file in c_files:\n",
    "        \n",
    "        with open(f\"files/c/codes/{c_file}\", 'r', encoding='utf-8', errors='ignore') as code_file:\n",
    "            lines = code_file.readlines()\n",
    "            num_lines = len(lines)\n",
    "            line_counts.append(num_lines)\n",
    "\n",
    "    if not line_counts:\n",
    "        return 0.0, 0.0  # No files found or no lines counted\n",
    "\n",
    "    avg = sum(line_counts) / len(line_counts)\n",
    "    variance = sum((x - avg) ** 2 for x in line_counts) / len(line_counts)\n",
    "    std_dev = math.sqrt(variance)\n",
    "\n",
    "    return avg, std_dev\n",
    "\n",
    "get_code_lines_stats(c_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_rules = {\n",
    "    \"c\": [\n",
    "        \"insecure-use-gets-fn\",\n",
    "        \"insecure-use-memset\",\n",
    "        \"insecure-use-printf-fn\",\n",
    "        \"insecure-use-strcat-fn\",\n",
    "        \"insecure-use-scanf-fn\",\n",
    "        \"insecure-use-string-copy-fn\",\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c 6\n"
     ]
    }
   ],
   "source": [
    "for key, value in allowed_rules.items():\n",
    "    print(key, len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated results by language and rule:\n",
      "Language: c\n",
      "  Rule: insecure-use-scanf-fn\n",
      "    CWE(s): CWE-676: Use of Potentially Dangerous Function\n",
      "    Unique conversation hashes: 378\n",
      "    Total occurrences: 1182\n",
      "  Rule: insecure-use-memset\n",
      "    CWE(s): CWE-14: Compiler Removal of Code to Clear Buffers\n",
      "    Unique conversation hashes: 117\n",
      "    Total occurrences: 263\n",
      "  Rule: insecure-use-string-copy-fn\n",
      "    CWE(s): CWE-676: Use of Potentially Dangerous Function\n",
      "    Unique conversation hashes: 120\n",
      "    Total occurrences: 273\n",
      "  Rule: insecure-use-strcat-fn\n",
      "    CWE(s): CWE-676: Use of Potentially Dangerous Function\n",
      "    Unique conversation hashes: 18\n",
      "    Total occurrences: 56\n",
      "  Rule: insecure-use-gets-fn\n",
      "    CWE(s): CWE-676: Use of Potentially Dangerous Function\n",
      "    Unique conversation hashes: 11\n",
      "    Total occurrences: 19\n",
      "  Rule: insecure-use-printf-fn\n",
      "    CWE(s): CWE-134: Use of Externally-Controlled Format String\n",
      "    Unique conversation hashes: 5\n",
      "    Total occurrences: 14\n",
      "  Overall unique conversation hashes (language): 581\n",
      "  Overall total occurrences (language): 1807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Flatten allowed rules into a list\n",
    "allowed_rules_list = [rule for sublist in allowed_rules.values() for rule in sublist]\n",
    "\n",
    "# Structure: language -> rule -> {count, hashes}\n",
    "language_rule_results = defaultdict(lambda: defaultdict(lambda: {\"count\": 0, \"hashes\": set()}))\n",
    "\n",
    "with open('codegrep_results.csv', mode='r') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    \n",
    "    for row in csv_reader:\n",
    "        error_id = row['error_id'].split('.')[-1]  # Extract last part of error_id\n",
    "        if error_id in allowed_rules_list:\n",
    "            language = row['language']  # Make sure this column exists in your CSV\n",
    "            conversation_hash = row['conversation_hash']\n",
    "            \n",
    "            # Update counts for language and rule\n",
    "            language_rule_results[language][error_id][\"count\"] += 1\n",
    "            language_rule_results[language][error_id][\"hashes\"].add(conversation_hash)\n",
    "\n",
    "print(\"Aggregated results by language and rule:\")\n",
    "total_all = 0\n",
    "for language, rules in language_rule_results.items():\n",
    "    total_unique_hashes = set()\n",
    "    total_count = 0\n",
    "    print(f\"Language: {language}\")\n",
    "    rule_id_to_cwes = walk_and_extract_cwe(rules)\n",
    "    for rule, data in rules.items():\n",
    "        unique_hash_count = len(data[\"hashes\"])\n",
    "        count = data[\"count\"]\n",
    "        total_unique_hashes.update(data[\"hashes\"])\n",
    "        total_count += count\n",
    "        cwes = rule_id_to_cwes.get(rule, [])\n",
    "        cwe_str = \", \".join(cwes) if cwes else \"N/A\"\n",
    "        print(f\"  Rule: {rule}\")\n",
    "        print(f\"    CWE(s): {cwe_str}\")\n",
    "        print(f\"    Unique conversation hashes: {unique_hash_count}\")\n",
    "        print(f\"    Total occurrences: {count}\")\n",
    "    total_all += len(total_unique_hashes)\n",
    "    print(f\"  Overall unique conversation hashes (language): {len(total_unique_hashes)}\")\n",
    "    print(f\"  Overall total occurrences (language): {total_count}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11261 581\n",
      "0.05159399698072995\n"
     ]
    }
   ],
   "source": [
    "total_files = set([c_file.split(\"_\")[0] for c_file in c_files])\n",
    "\n",
    "print(len(total_files), total_all)\n",
    "print(total_all / len(total_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unique_hashes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 39\u001b[0m\n\u001b[1;32m     35\u001b[0m     std_dev \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(variance)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m avg, std_dev\n\u001b[0;32m---> 39\u001b[0m good_avg, good_std \u001b[38;5;241m=\u001b[39m get_code_lines_stats(c_files, \u001b[43munique_hashes\u001b[49m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGood results ||| Avg lines: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgood_avg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Std: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgood_std\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m bad_avg, bad_std \u001b[38;5;241m=\u001b[39m get_code_lines_stats(c_files, unique_hashes, include_only_unique\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'unique_hashes' is not defined"
     ]
    }
   ],
   "source": [
    "def get_code_lines_stats(c_files, unique_hashes=None, include_only_unique=False):\n",
    "    if unique_hashes is None:\n",
    "        unique_hashes = set()\n",
    "\n",
    "    line_counts = []\n",
    "\n",
    "    for c_file in c_files:\n",
    "        convo_hash = c_file.split(\"_\")[0]\n",
    "    \n",
    "        if include_only_unique:\n",
    "            # Only process rows whose convo_hash is in unique_hashes\n",
    "            if convo_hash not in unique_hashes:\n",
    "                continue\n",
    "        else:\n",
    "            # Exclude rows whose convo_hash is in unique_hashes\n",
    "            if convo_hash in unique_hashes:\n",
    "                continue\n",
    "\n",
    "        filename = f\"files/c/codes/{c_file}\"\n",
    "\n",
    "        if not os.path.isfile(filename):\n",
    "            print(f\"File {filename} not found.\")\n",
    "            continue\n",
    "\n",
    "        with open(filename, 'r', encoding='utf-8', errors='ignore') as code_file:\n",
    "            lines = code_file.readlines()\n",
    "            num_lines = len(lines)\n",
    "            line_counts.append(num_lines)\n",
    "\n",
    "    if not line_counts:\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    avg = sum(line_counts) / len(line_counts)\n",
    "    variance = sum((x - avg) ** 2 for x in line_counts) / len(line_counts)\n",
    "    std_dev = math.sqrt(variance)\n",
    "\n",
    "    return avg, std_dev\n",
    "\n",
    "good_avg, good_std = get_code_lines_stats(c_files, unique_hashes)\n",
    "print(f\"Good results ||| Avg lines: {good_avg} | Std: {good_std} \")\n",
    "\n",
    "\n",
    "bad_avg, bad_std = get_code_lines_stats(c_files, unique_hashes, include_only_unique=True)\n",
    "\n",
    "print(f\"Bad results ||| Avg lines: {bad_avg} | Std: {bad_std} \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the results in a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/unsafe_memory_occurrence.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Rule\", \"Unique Hash Count\", \"Hashes\"])\n",
    "    for rule, data in results.items():\n",
    "        writer.writerow([\n",
    "            rule,\n",
    "            len(data[\"hashes\"]),\n",
    "            \";\".join(data[\"hashes\"])\n",
    "        ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
