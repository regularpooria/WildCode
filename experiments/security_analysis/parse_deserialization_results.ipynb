{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "from collections import defaultdict\n",
        "import os\n",
        "import math\n",
        "from utils.extractor import walk_and_extract_cwe\n",
        "\n",
        "lang_to_ext = {\n",
        "    'c': 'c',\n",
        "    'cpp': 'cpp',\n",
        "    'python': 'py',\n",
        "    'java': 'java',\n",
        "    'javascript': 'js',\n",
        "    'php': 'php',\n",
        "    \"csharp\": \"cs\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_conversations(file_path):\n",
        "    # Structure: {language: {conversation_hash: count, 'total_unique': count, 'total_occurrences': count}}\n",
        "    language_data = defaultdict(lambda: {'unique_conversations': set(), 'total_occurrences': 0})\n",
        "\n",
        "    with open(file_path, mode='r') as csv_file:\n",
        "        csv_reader = csv.DictReader(csv_file)\n",
        "\n",
        "        for row in csv_reader:\n",
        "            conversation_hash = row['conversation_hash']\n",
        "            language = row['language']\n",
        "\n",
        "            # Track data per language\n",
        "            lang_data = language_data[language]\n",
        "\n",
        "            # Add to unique conversations if not already present\n",
        "            if conversation_hash not in lang_data['unique_conversations']:\n",
        "                lang_data['unique_conversations'].add(conversation_hash)\n",
        "\n",
        "            # Increment total occurrences\n",
        "            lang_data['total_occurrences'] += 1\n",
        "\n",
        "    # Convert sets to counts and prepare final output\n",
        "    result = {}\n",
        "    for language, data in language_data.items():\n",
        "        result[language] = {\n",
        "            'unique_conversations': len(data['unique_conversations']),\n",
        "            'total_occurrences': data['total_occurrences']\n",
        "        }\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'csharp': {'unique_conversations': 56, 'total_occurrences': 82}, 'java': {'unique_conversations': 273, 'total_occurrences': 634}, 'javascript': {'unique_conversations': 463, 'total_occurrences': 805}, 'python': {'unique_conversations': 2258, 'total_occurrences': 4833}}\n",
            "Unique conversations and total occurrences per language:\n",
            "Language: csharp\n",
            "  Unique conversations: 56\n",
            "  Total occurrences: 82\n",
            "Language: java\n",
            "  Unique conversations: 273\n",
            "  Total occurrences: 634\n",
            "Language: javascript\n",
            "  Unique conversations: 463\n",
            "  Total occurrences: 805\n",
            "Language: python\n",
            "  Unique conversations: 2258\n",
            "  Total occurrences: 4833\n"
          ]
        }
      ],
      "source": [
        "file_path = 'opengrep_results_deserialization.csv'\n",
        "language_conversations = count_conversations(file_path)\n",
        "print(language_conversations)\n",
        "unique_convo_hash_good = 0\n",
        "# Print results\n",
        "print(\"Unique conversations and total occurrences per language:\")\n",
        "for language, data in language_conversations.items():\n",
        "    print(f\"Language: {language}\")\n",
        "    unique_convo_hash_good += data['unique_conversations']\n",
        "    print(f\"  Unique conversations: {data['unique_conversations']}\")\n",
        "    print(f\"  Total occurrences: {data['total_occurrences']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "allowed_rules = {\n",
        "    \"java\": [\n",
        "        \"insecure-jms-deserialization\",\n",
        "        \"object-deserialization\",\n",
        "        \"jackson-unsafe-deserialization\",\n",
        "        \"insecure-resteasy-deserialization\",\n",
        "        \"use-snakeyaml-constructor\",\n",
        "        \"xmlinputfactory-external-entities-enabled\",\n",
        "        \"xmlinputfactory-possible-xxe\",\n",
        "        \"documentbuilderfactory-disallow-doctype-decl-false\",\n",
        "        \"documentbuilderfactory-disallow-doctype-decl-missing\",\n",
        "        \"documentbuilderfactory-external-general-entities-true\",\n",
        "        \"documentbuilderfactory-external-parameter-entities-true\",\n",
        "        \"saxparserfactory-disallow-doctype-decl-missing\",\n",
        "        \"transformerfactory-dtds-not-disabled\",\n",
        "        \"server-dangerous-class-deserialization\",\n",
        "        \"server-dangerous-object-deserialization\",\n",
        "    ],\n",
        "    \"csharp\": [\n",
        "        \"insecure-binaryformatalter-deseriization\",\n",
        "        \"data-contract-resolver\",\n",
        "        \"insecure-fastjson-deserialization\",\n",
        "        \"insecure-fspickler-deserialization\",\n",
        "        \"insecure-typefilterlevel-full\",\n",
        "        \"insecure-javascriptserializer-deserialization\",\n",
        "        \"insecure-losformatter-deserialization\",\n",
        "        \"insecure-netdatacontract-deserialization\",\n",
        "        \"insecure-newtonsoft-deserialization\",\n",
        "        \"insecure-soapformatter-deserialization\",\n",
        "        \"insecure-typefilterlevel-full\",\n",
        "    ],\n",
        "    \"javascript\": [\n",
        "        \"express-expat-xxe\",\n",
        "        \"express-xml2json-xxe\",\n",
        "        \"express-third-party-object-deserialization\",\n",
        "        \"grpc-nodejs-insecure-connection\",\n",
        "        \"xml2json-xxe\",\n",
        "    ],\n",
        "    \"python\": [\n",
        "        \"tainted-pickle-deserialization\",\n",
        "        \"avoid-insecure-deserialization\",\n",
        "        \"insecure-deserialization\",\n",
        "        \"multiprocessing-recv\",\n",
        "        \"marshal-usage\",\n",
        "        \"avoid-jsonpickle\",\n",
        "        \"avoid-pyyaml-load\",\n",
        "        \"avoid-unsafe-ruamel\",\n",
        "        \n",
        "    ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "java 15\n",
            "csharp 11\n",
            "javascript 5\n",
            "python 8\n"
          ]
        }
      ],
      "source": [
        "for key, value in allowed_rules.items():\n",
        "    print(key, len(value))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Aggregated results by language and rule:\n",
            "Language: java\n",
            "  Rule: documentbuilderfactory-disallow-doctype-decl-missing\n",
            "    CWE(s): CWE-611: Improper Restriction of XML External Entity Reference\n",
            "    Unique conversation hashes: 3\n",
            "    Total occurrences: 3\n",
            "  Rule: object-deserialization\n",
            "    CWE(s): CWE-502: Deserialization of Untrusted Data\n",
            "    Unique conversation hashes: 21\n",
            "    Total occurrences: 33\n",
            "  Rule: transformerfactory-dtds-not-disabled\n",
            "    CWE(s): CWE-611: Improper Restriction of XML External Entity Reference\n",
            "    Unique conversation hashes: 1\n",
            "    Total occurrences: 1\n",
            "  Rule: saxparserfactory-disallow-doctype-decl-missing\n",
            "    CWE(s): CWE-611: Improper Restriction of XML External Entity Reference\n",
            "    Unique conversation hashes: 5\n",
            "    Total occurrences: 7\n",
            "  Rule: use-snakeyaml-constructor\n",
            "    CWE(s): CWE-502: Deserialization of Untrusted Data\n",
            "    Unique conversation hashes: 1\n",
            "    Total occurrences: 3\n",
            "  Overall unique conversation hashes (language): 30\n",
            "  Overall total occurrences (language): 47\n",
            "  Percentage of unique conversations that are wrong: 10.99%\n",
            "\n",
            "\n",
            "Language: javascript\n",
            "  Rule: grpc-nodejs-insecure-connection\n",
            "    CWE(s): CWE-502: Deserialization of Untrusted Data\n",
            "    Unique conversation hashes: 7\n",
            "    Total occurrences: 20\n",
            "  Overall unique conversation hashes (language): 7\n",
            "  Overall total occurrences (language): 20\n",
            "  Percentage of unique conversations that are wrong: 1.51%\n",
            "\n",
            "\n",
            "Language: python\n",
            "  Rule: marshal-usage\n",
            "    CWE(s): CWE-502: Deserialization of Untrusted Data\n",
            "    Unique conversation hashes: 2\n",
            "    Total occurrences: 3\n",
            "  Overall unique conversation hashes (language): 2\n",
            "  Overall total occurrences (language): 3\n",
            "  Percentage of unique conversations that are wrong: 0.09%\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Flatten allowed rules into a list\n",
        "allowed_rules_list = [rule for sublist in allowed_rules.values() for rule in sublist]\n",
        "\n",
        "# Structure: language -> rule -> {count, hashes}\n",
        "language_rule_results = defaultdict(lambda: defaultdict(lambda: {\"count\": 0, \"hashes\": set()}))\n",
        "\n",
        "with open('opengrep_results.csv', mode='r') as csv_file:\n",
        "    csv_reader = csv.DictReader(csv_file)\n",
        "    \n",
        "    for row in csv_reader:\n",
        "        error_id = row['error_id'].split('.')[-1]  # Extract last part of error_id\n",
        "        if error_id in allowed_rules_list:\n",
        "            language = row['language']  # Make sure this column exists in your CSV\n",
        "            conversation_hash = row['conversation_hash']\n",
        "            \n",
        "            # Update counts for language and rule\n",
        "            language_rule_results[language][error_id][\"count\"] += 1\n",
        "            language_rule_results[language][error_id][\"hashes\"].add(conversation_hash)\n",
        "\n",
        "print(\"Aggregated results by language and rule:\")\n",
        "for language, rules in language_rule_results.items():\n",
        "    total_unique_hashes = set()\n",
        "    total_count = 0\n",
        "    print(f\"Language: {language}\")\n",
        "    rule_id_to_cwes = walk_and_extract_cwe(rules)\n",
        "    for rule, data in rules.items():\n",
        "        unique_hash_count = len(data[\"hashes\"])\n",
        "        count = data[\"count\"]\n",
        "        total_unique_hashes.update(data[\"hashes\"])\n",
        "        total_count += count\n",
        "        cwes = rule_id_to_cwes.get(rule, [])\n",
        "        cwe_str = \", \".join(cwes) if cwes else \"N/A\"\n",
        "        print(f\"  Rule: {rule}\")\n",
        "        print(f\"    CWE(s): {cwe_str}\")\n",
        "        print(f\"    Unique conversation hashes: {unique_hash_count}\")\n",
        "        print(f\"    Total occurrences: {count}\")\n",
        "        \n",
        "    num_all_hashes = language_conversations.get(language, {}).get('unique_conversations', 0)\n",
        "    num_bad_hashes = len(total_unique_hashes)    \n",
        "    percentage_bad = (num_bad_hashes / num_all_hashes * 100) if num_all_hashes > 0 else 0\n",
        "    print(f\"  Overall unique conversation hashes (language): {len(total_unique_hashes)}\")\n",
        "    print(f\"  Overall total occurrences (language): {total_count}\")\n",
        "    print(f\"  Percentage of unique conversations that are wrong: {percentage_bad:.2f}%\\n\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Total statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3050 40\n",
            "0.013114754098360656\n"
          ]
        }
      ],
      "source": [
        "# Flatten the allowed rules into a single list\n",
        "allowed_rules_list = [rule for sublist in allowed_rules.values() for rule in sublist]\n",
        "results = defaultdict(lambda: {\"count\": 0, \"hashes\": set()})\n",
        "# Read the CSV file\n",
        "with open('opengrep_results.csv', mode='r') as csv_file:\n",
        "    csv_reader = csv.DictReader(csv_file)\n",
        "\n",
        "    for row in csv_reader:\n",
        "        error_id = row['error_id'].split('.')[-1]  # Extract the last part of the error_id\n",
        "        if error_id in allowed_rules_list:\n",
        "            conversation_hash = row['conversation_hash']\n",
        "            if error_id in results:\n",
        "                # Update the count and add the hash to the set\n",
        "                results[error_id][\"count\"] += 1\n",
        "                results[error_id][\"hashes\"].add(conversation_hash)\n",
        "            else:\n",
        "                # Initialize a new entry\n",
        "                results[error_id] = {\"count\": 1, \"hashes\": {conversation_hash}}         \n",
        "            \n",
        "unique_convo_hash_bad = 0\n",
        "for rule in results:\n",
        "    count = results[rule][\"count\"]\n",
        "    unique_hashes = results[rule][\"hashes\"]\n",
        "    unique_convo_hash_bad += len(unique_hashes)\n",
        "print(unique_convo_hash_good, unique_convo_hash_bad)\n",
        "print(unique_convo_hash_bad / unique_convo_hash_good)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Good results ||| Avg lines: 49.200566750629726 | Std: 45.145936880380155 \n",
            "Bad results ||| Avg lines: 26.0 | Std: 1.0 \n"
          ]
        }
      ],
      "source": [
        "def get_code_lines_stats(csv_path, unique_hashes=None, include_only_unique=False):\n",
        "    if unique_hashes is None:\n",
        "        unique_hashes = set()\n",
        "\n",
        "    line_counts = []\n",
        "\n",
        "    with open(csv_path, newline='') as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for row in reader:\n",
        "            convo_hash = row['conversation_hash']\n",
        "\n",
        "            if include_only_unique:\n",
        "                # Only process rows whose convo_hash is in unique_hashes\n",
        "                if convo_hash not in unique_hashes:\n",
        "                    continue\n",
        "            else:\n",
        "                # Exclude rows whose convo_hash is in unique_hashes\n",
        "                if convo_hash in unique_hashes:\n",
        "                    continue\n",
        "\n",
        "            code_index = row['code_index']\n",
        "            language = row['language'].lower()\n",
        "\n",
        "            ext = lang_to_ext.get(language)\n",
        "            if not ext:\n",
        "                print(f\"Unknown language '{language}' for conversation_hash={convo_hash}\")\n",
        "                continue\n",
        "\n",
        "            filename = f\"files/{language}/codes/{convo_hash}_{code_index}.{ext}\"\n",
        "\n",
        "            if not os.path.isfile(filename):\n",
        "                print(f\"File {filename} not found.\")\n",
        "                continue\n",
        "\n",
        "            with open(filename, 'r', encoding='utf-8', errors='ignore') as code_file:\n",
        "                lines = code_file.readlines()\n",
        "                num_lines = len(lines)\n",
        "                line_counts.append(num_lines)\n",
        "\n",
        "    if not line_counts:\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    avg = sum(line_counts) / len(line_counts)\n",
        "    variance = sum((x - avg) ** 2 for x in line_counts) / len(line_counts)\n",
        "    std_dev = math.sqrt(variance)\n",
        "\n",
        "    return avg, std_dev\n",
        "\n",
        "good_avg, good_std = get_code_lines_stats(file_path, unique_hashes)\n",
        "print(f\"Good results ||| Avg lines: {good_avg} | Std: {good_std} \")\n",
        "\n",
        "\n",
        "bad_avg, bad_std = get_code_lines_stats(file_path, unique_hashes, include_only_unique=True)\n",
        "print(f\"Bad results ||| Avg lines: {bad_avg} | Std: {bad_std} \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save the results in a CSV file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"results/weak_random_occurrences.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Rule\", \"Unique Hash Count\", \"Hashes\"])\n",
        "    for rule, data in results.items():\n",
        "        writer.writerow([\n",
        "            rule,\n",
        "            len(data[\"hashes\"]),\n",
        "            \";\".join(data[\"hashes\"])\n",
        "        ])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
