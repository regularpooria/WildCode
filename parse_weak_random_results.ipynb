{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import math\n",
    "from utils.extractor import walk_and_extract_cwe\n",
    "\n",
    "lang_to_ext = {\n",
    "    'c': 'c',\n",
    "    'cpp': 'cpp',\n",
    "    'python': 'py',\n",
    "    'java': 'java',\n",
    "    'javascript': 'js',\n",
    "    'php': 'php',\n",
    "    \"csharp\": \"cs\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_conversations(file_path):\n",
    "    # Structure: {language: {conversation_hash: count, 'total_unique': count, 'total_occurrences': count}}\n",
    "    language_data = defaultdict(lambda: {'unique_conversations': set(), 'total_occurrences': 0})\n",
    "\n",
    "    with open(file_path, mode='r') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "\n",
    "        for row in csv_reader:\n",
    "            conversation_hash = row['conversation_hash']\n",
    "            language = row['language']\n",
    "\n",
    "            # Track data per language\n",
    "            lang_data = language_data[language]\n",
    "\n",
    "            # Add to unique conversations if not already present\n",
    "            if conversation_hash not in lang_data['unique_conversations']:\n",
    "                lang_data['unique_conversations'].add(conversation_hash)\n",
    "\n",
    "            # Increment total occurrences\n",
    "            lang_data['total_occurrences'] += 1\n",
    "\n",
    "    # Convert sets to counts and prepare final output\n",
    "    result = {}\n",
    "    for language, data in language_data.items():\n",
    "        result[language] = {\n",
    "            'unique_conversations': len(data['unique_conversations']),\n",
    "            'total_occurrences': data['total_occurrences']\n",
    "        }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'c': {'unique_conversations': 203, 'total_occurrences': 1148}, 'csharp': {'unique_conversations': 91, 'total_occurrences': 133}, 'java': {'unique_conversations': 150, 'total_occurrences': 232}, 'javascript': {'unique_conversations': 352, 'total_occurrences': 987}, 'python': {'unique_conversations': 2810, 'total_occurrences': 12791}}\n",
      "Unique conversations and total occurrences per language:\n",
      "Language: c\n",
      "  Unique conversations: 203\n",
      "  Total occurrences: 1148\n",
      "Language: csharp\n",
      "  Unique conversations: 91\n",
      "  Total occurrences: 133\n",
      "Language: java\n",
      "  Unique conversations: 150\n",
      "  Total occurrences: 232\n",
      "Language: javascript\n",
      "  Unique conversations: 352\n",
      "  Total occurrences: 987\n",
      "Language: python\n",
      "  Unique conversations: 2810\n",
      "  Total occurrences: 12791\n"
     ]
    }
   ],
   "source": [
    "file_path = 'codegrep_results_random.csv'\n",
    "language_conversations = count_conversations(file_path)\n",
    "print(language_conversations)\n",
    "unique_convo_hash_good = 0\n",
    "# Print results\n",
    "print(\"Unique conversations and total occurrences per language:\")\n",
    "for language, data in language_conversations.items():\n",
    "    print(f\"Language: {language}\")\n",
    "    unique_convo_hash_good += data['unique_conversations']\n",
    "    print(f\"  Unique conversations: {data['unique_conversations']}\")\n",
    "    print(f\"  Total occurrences: {data['total_occurrences']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_rules = {\n",
    "    \"java\": [\n",
    "        \"weak-random\"\n",
    "    ],\n",
    "    \"csharp\": [\n",
    "        \"use_weak_rng_for_keygeneration\",\n",
    "    ],\n",
    "    \"javascript\": [\n",
    "        \"JS_WEAK_RNG\",\n",
    "    ],\n",
    "    \"python\": [\n",
    "        \"PYTHON_WEAK_RNG\",\n",
    "        \"PYTHON_WEAK_RNG_UNQUALIFIED\",\n",
    "        \"PYTHON_WEAK_RNG_WRAPPER\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java 1\n",
      "csharp 1\n",
      "javascript 1\n",
      "python 3\n"
     ]
    }
   ],
   "source": [
    "for key, value in allowed_rules.items():\n",
    "    print(key, len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated results by language and rule:\n",
      "Language: java\n",
      "  Rule: weak-random\n",
      "    CWE(s): CWE-330: Use of Insufficiently Random Values\n",
      "    Unique conversation hashes: 15\n",
      "    Total occurrences: 29\n",
      "  Overall unique conversation hashes (language): 15\n",
      "  Overall total occurrences (language): 29\n",
      "  Percentage of unique conversations that are wrong: 10.00%\n",
      "\n",
      "\n",
      "Language: python\n",
      "  Rule: PYTHON_WEAK_RNG_UNQUALIFIED\n",
      "    CWE(s): N/A\n",
      "    Unique conversation hashes: 2\n",
      "    Total occurrences: 2\n",
      "  Overall unique conversation hashes (language): 2\n",
      "  Overall total occurrences (language): 2\n",
      "  Percentage of unique conversations that are wrong: 0.07%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Flatten allowed rules into a list\n",
    "allowed_rules_list = [rule for sublist in allowed_rules.values() for rule in sublist]\n",
    "\n",
    "# Structure: language -> rule -> {count, hashes}\n",
    "language_rule_results = defaultdict(lambda: defaultdict(lambda: {\"count\": 0, \"hashes\": set()}))\n",
    "\n",
    "with open('codegrep_results.csv', mode='r') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    \n",
    "    for row in csv_reader:\n",
    "        error_id = row['error_id'].split('.')[-1]  # Extract last part of error_id\n",
    "        if error_id in allowed_rules_list:\n",
    "            language = row['language']  # Make sure this column exists in your CSV\n",
    "            conversation_hash = row['conversation_hash']\n",
    "            \n",
    "            # Update counts for language and rule\n",
    "            language_rule_results[language][error_id][\"count\"] += 1\n",
    "            language_rule_results[language][error_id][\"hashes\"].add(conversation_hash)\n",
    "total_all = 0\n",
    "print(\"Aggregated results by language and rule:\")\n",
    "for language, rules in language_rule_results.items():\n",
    "    total_unique_hashes = set()\n",
    "    total_count = 0\n",
    "    print(f\"Language: {language}\")\n",
    "    rule_id_to_cwes = walk_and_extract_cwe(rules)\n",
    "    for rule, data in rules.items():\n",
    "        unique_hash_count = len(data[\"hashes\"])\n",
    "        count = data[\"count\"]\n",
    "        total_unique_hashes.update(data[\"hashes\"])\n",
    "        total_count += count\n",
    "        cwes = rule_id_to_cwes.get(rule, [])\n",
    "        cwe_str = \", \".join(cwes) if cwes else \"N/A\"\n",
    "        print(f\"  Rule: {rule}\")\n",
    "        print(f\"    CWE(s): {cwe_str}\")\n",
    "        print(f\"    Unique conversation hashes: {unique_hash_count}\")\n",
    "        print(f\"    Total occurrences: {count}\")\n",
    "        \n",
    "    num_all_hashes = language_conversations.get(language, {}).get('unique_conversations', 0)\n",
    "    num_bad_hashes = len(total_unique_hashes)    \n",
    "    percentage_bad = (num_bad_hashes / num_all_hashes * 100) if num_all_hashes > 0 else 0\n",
    "    total_all += len(total_unique_hashes)\n",
    "    print(f\"  Overall unique conversation hashes (language): {len(total_unique_hashes)}\")\n",
    "    print(f\"  Overall total occurrences (language): {total_count}\")\n",
    "    print(f\"  Percentage of unique conversations that are wrong: {percentage_bad:.2f}%\\n\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3606 17\n",
      "0.004714364947310039\n"
     ]
    }
   ],
   "source": [
    "print(unique_convo_hash_good, total_all)\n",
    "print(total_all / unique_convo_hash_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unique_hashes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 49\u001b[0m\n\u001b[1;32m     45\u001b[0m     std_dev \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(variance)\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m avg, std_dev\n\u001b[0;32m---> 49\u001b[0m good_avg, good_std \u001b[38;5;241m=\u001b[39m get_code_lines_stats(file_path, \u001b[43munique_hashes\u001b[49m)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGood results ||| Avg lines: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgood_avg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Std: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgood_std\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m bad_avg, bad_std \u001b[38;5;241m=\u001b[39m get_code_lines_stats(file_path, unique_hashes, include_only_unique\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'unique_hashes' is not defined"
     ]
    }
   ],
   "source": [
    "def get_code_lines_stats(csv_path, unique_hashes=None, include_only_unique=False):\n",
    "    if unique_hashes is None:\n",
    "        unique_hashes = set()\n",
    "\n",
    "    line_counts = []\n",
    "\n",
    "    with open(csv_path, newline='') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            convo_hash = row['conversation_hash']\n",
    "\n",
    "            if include_only_unique:\n",
    "                # Only process rows whose convo_hash is in unique_hashes\n",
    "                if convo_hash not in unique_hashes:\n",
    "                    continue\n",
    "            else:\n",
    "                # Exclude rows whose convo_hash is in unique_hashes\n",
    "                if convo_hash in unique_hashes:\n",
    "                    continue\n",
    "\n",
    "            code_index = row['code_index']\n",
    "            language = row['language'].lower()\n",
    "\n",
    "            ext = lang_to_ext.get(language)\n",
    "            if not ext:\n",
    "                print(f\"Unknown language '{language}' for conversation_hash={convo_hash}\")\n",
    "                continue\n",
    "\n",
    "            filename = f\"files/{language}/codes/{convo_hash}_{code_index}.{ext}\"\n",
    "\n",
    "            if not os.path.isfile(filename):\n",
    "                print(f\"File {filename} not found.\")\n",
    "                continue\n",
    "\n",
    "            with open(filename, 'r', encoding='utf-8', errors='ignore') as code_file:\n",
    "                lines = code_file.readlines()\n",
    "                num_lines = len(lines)\n",
    "                line_counts.append(num_lines)\n",
    "\n",
    "    if not line_counts:\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    avg = sum(line_counts) / len(line_counts)\n",
    "    variance = sum((x - avg) ** 2 for x in line_counts) / len(line_counts)\n",
    "    std_dev = math.sqrt(variance)\n",
    "\n",
    "    return avg, std_dev\n",
    "\n",
    "good_avg, good_std = get_code_lines_stats(file_path, unique_hashes)\n",
    "print(f\"Good results ||| Avg lines: {good_avg} | Std: {good_std} \")\n",
    "\n",
    "\n",
    "bad_avg, bad_std = get_code_lines_stats(file_path, unique_hashes, include_only_unique=True)\n",
    "print(f\"Bad results ||| Avg lines: {bad_avg} | Std: {bad_std} \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the results in a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/weak_random_occurrences.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Rule\", \"Unique Hash Count\", \"Hashes\"])\n",
    "    for rule, data in results.items():\n",
    "        writer.writerow([\n",
    "            rule,\n",
    "            len(data[\"hashes\"]),\n",
    "            \";\".join(data[\"hashes\"])\n",
    "        ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
